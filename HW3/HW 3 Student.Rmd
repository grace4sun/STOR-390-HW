---
title: "HW 3"
author: "Grace Sun"
date: "2/28/2024"
output: 
  html_document:
    number_sections: true
---

# 

In this homework, we will discuss support vector machines and tree-based methods. I will begin by simulating some data for you to use with SVM.

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```

## 

Quite clearly, the above data is not linearly separable. Create a training-testing partition with 100 random observations in the training partition. Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$. Plot the svm on the training data.

```{r}
set.seed(1) 
train <- sample(1:nrow(x), 100)

# Create training data partition
x_train <- x[train,]
y_train <- y[train]
dat_train = data.frame(x_train, y = as.factor(y_train))

# Create testing data partition
x_test <- x[-train,]
y_test <- y[-train]
dat_test = data.frame(x_test, y = as.factor(y_test))

# Fit an SVM on training data
svmfit = svm(y~., data = dat_train, kernel = "radial", cost = 1, gamma = 1, scale = FALSE)
print(svmfit)

# Grid function
make.grid = function(x, n = 75) {
  grange = apply(x, 2, range)
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x_train)

# Overlay prediction on the grid
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x_train, col = y_train*2, pch = 19) # Since classes are 1 and 2, and red and blue correlate to 2 and 4 in R, multiply by 2 for color.
points(x_train[svmfit$index,], pch = 5, cex = 2)
```

## 

Notice that the above decision boundary is decidedly non-linear. It seems to perform reasonably well, but there are indeed some misclassifications. Let's see if increasing the cost [^1] helps our classification error rate. Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000. Plot this svm on the training data.

[^1]: Remember this is a parameter that decides how smooth your decision boundary should be

```{r}
# Fit SVM with cost of 10000
svmfit = svm(y~., data = dat_train, kernel = "radial", cost = 10000, gamma = 1, scale = FALSE)
print(svmfit)

xgrid = make.grid(x_train)

# Overlay prediction on the grid
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x_train, col = y_train*2, pch = 19)
points(x_train[svmfit$index,], pch = 5, cex = 2)
```

## 

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model.

This model is overfit. The primary danger of overfitting a model is the decrease in the generalizability of the model. Since it is overly fit to the noise in the training set, it will likely not be able to make accurate predictions on a testing data set. 

## 

Create a confusion matrix by using this svm to predict on the current testing partition. Comment on the confusion matrix. Is there any disparity in our classification results?

```{r}
table(true=y_test, pred=predict(svmfit, newdata=dat_test))
```
There does seems to be a disparity in the classifications as shown in the confusion matrix. Observations in class 1 were much more likely to be misclassified as class 2 than observations in class 2 were to be misclassified as class 1. Respectively, 17/79 = 21.5% of points in class 1 were misclassified, and 3/21 = 14.3% of points in class 2 were misclassified.

## 

Is this disparity because of imbalance in the training/testing partition? Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25% of class 2 in the data as a whole.

```{r}
table(y_train)
```

It looks like 29% of the training dataset is of class 2. This is not too far off from the general 25% of class 2 in the data as a whole. Although a non representative training data set can cause a disparity, this proportion seems acceptable, and the misclassification could instead be due to overfitting of the model, rather than a non-representative training dataset.

## 

Let's try and balance the above to solutions via cross-validation. Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}. Save the output of this function in a variable called `tune.out`.

```{r}
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat_train, kernel = "radial", 
                 ranges = list(cost =  c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)), scale = FALSE)
tune.out
```

I will take `tune.out` and use the best model according to error rate to test on our data. I will report a confusion matrix corresponding to the 100 predictions.

```{r, eval = FALSE}
table(true=y_test, pred=predict(tune.out$best.model, newdata=dat_test))
```

## 

Comment on the confusion matrix. How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.

While the misclassification rate for class 2 is still exactly the same, the misclassification rate for class 1 has decreased from 17/79 = 21.5% to 6/79 = 7.5%. This model is improved from the model in question 2 and better at classification on a testing data set since it is not overfit, with a reasonable cost of 1, rather than an astronomically high cost of 10000. However, we still need to ensure it is generalizable and that the training dataset is a good representation for the testing dataset.

# 

Let's turn now to decision trees.

```{r}

library(kmed)
data(heart)
library(tree)

```

## 

The response variable is currently a categorical variable with four levels. Convert heart disease into binary categorical variable. Then, ensure that it is properly stored as a factor.

```{r}
heart_disease = as.factor(ifelse(heart$class == 0, "0", "1"))
heart_data = data.frame(heart, heart_disease)
head(heart_data)
```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you). Plot the tree.

```{r}
set.seed(101)
train_ran <- sample(1:nrow(heart_data), 240)
heart_train <- heart_data[train_ran,]
heart.tree = tree(heart_disease~. -class, heart_train)
plot(heart.tree)
text(heart.tree, pretty = 0)
```

## 

Use the trained model to classify the remaining testing points. Create a confusion matrix to evaluate performance. Report the classification error rate.

```{r}
heart.pred = predict(heart.tree, heart_data[-train_ran,], type="class")
with(heart_data[-train_ran,], table(heart.pred, heart_disease))

# Classification error rate:
(8+3)/(28+3+8+18)
```

## 

Above we have a fully grown (bushy) tree. Now, cross validate it using the `cv.tree` command. Specify cross validation to be done according to the misclassification rate. Choose an ideal number of splits, and plot this tree. Finally, use this pruned tree to test on the testing set. Report a confusion matrix and the misclassification rate.

```{r}
set.seed(101)
cv.heart <- cv.tree(heart.tree, FUN = prune.misclass)
cv.heart

# Best choice is 2 splits
plot(cv.heart$size, cv.heart$dev, type = "b")

prune.heart = prune.misclass(heart.tree, best = 2)

plot(prune.heart)
text(prune.heart, pretty=0)

tree.pred = predict(prune.heart, heart_data[-train_ran,], type="class")
with(heart_data[-train_ran,], table(tree.pred, heart_disease))

# Misclassification rate:
(8+9)/(28+9+8+12)

```

## 

Discuss the trade-off in accuracy and interpretability in pruning the above tree.

Pruning the above tree improves interpretability, but it also increases the misclassification rate. However, the vast improvement in interpretability is a good trade off for a small decrease in accuracy. The original bushy tree was barely even readable! A bushy tree that is overfit can also result in a non generalizable tree for testing or other datasets.

## 

Discuss the ways a decision tree could manifest algorithmic bias.

If the training dataset reflects prejudices or if certain classes are underrepresented, algorithmic bias may manifest when training a model. Since the premise of decision trees is classifying using labels that separate the data well, algorithmic bias can manifest when all observations under a particular class label are split off into their own subtree, which may amplify existing bias in the data. When the model is used on testing datasets, classes that are not well represented in the training data might have particularly high misclassification rates, disproportionate to other classes that are well represented.
