---
title: "HW 2 Student"
author: "Grace Sun"
date: "1/14/24"
output: 
  html_document:
    number_sections: true
---

This homework is meant to illustrate the methods of classification algorithms as well as their potential pitfalls.  In class, we demonstrated K-Nearest-Neighbors using the `iris` dataset.  Today I will give you a different subset of this same data, and you will train a KNN classifier.  

```{r, echo = FALSE}
set.seed(123)
library(class)

df <- data(iris) 

normal <-function(x) {
  (x -min(x))/(max(x)-min(x))   
}

iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], normal))

subset <- c(1:45, 58, 60:70, 82, 94, 110:150)
iris_train <- iris_norm[subset,] 
iris_test <- iris_norm[-subset,] 

iris_target_category <- iris[subset,5]
iris_test_category <- iris[-subset,5]


```

#
Above, I have given you a training-testing partition.  Train the KNN with $K = 5$ on the training data and use this to classify the 50 test observations.  Once you have classified the test observations, create a contingency table -- like we did in class -- to evaluate which observations your algorithm is misclassifying.   

```{r}
set.seed(123)

pr <- knn(iris_train, iris_test, cl= iris_target_category, k=5)
tab <- table(pr, iris_test_category)

tab

accuracy <- function(x){
  sum(diag(x))/(sum(rowSums(x)))*100
}
accuracy(tab)
```

#

Discuss your results.  If you have done this correctly, you should have a classification error rate that is roughly 20% higher than what we observed in class.  Why is this the case? In particular run a summary of the `iris_test_category` as well as `iris_target_category` and discuss how this plays a role in your answer.  

```{r}
summary(iris_test_category)
summary(iris_target_category)
```
Out of 36 versicolor in the testing data set, 25 were classified correctly and 11 were classified incorrectly as virignica. All of the setosa and virginica in the testing set were correctly classified. The classification error rate is much higher here because the training data set is not a good representation of the testing data set. The training data is imbalanced, with a much higher number of setosa (45) and virginica (41) than versicolor (14). Since KNN classification relies on looking at the closest data points in the training set from each of the points in the testing dataset, the misclassification rate can be high when there are significantly more points from some categories and less of others simply because it is probabilistically more likely that more data points from a category with more representation are nearby. When looking for the 5 nearest neighbors to classify a versicolor in the testing set, it's likely that most of the nearest neighbors are of virginica simply because of the sheer number of viriginca points that are in the training dataset and because the two already overlap slightly, as compared to setosa which is farther away. The imbalanced training set causes the higher error rate, since it did not contain enough examples of versicolor to allow KNN to correctly identify the distinctions between versicolor and virginica. This emphasizes the importance of having a well balanced training set that is representative of all of the categories in the dataset.

#

Build a github repository to store your homework assignments.  Share the link in this file.  

https://github.com/grace4sun/STOR-390-HW/tree/main
